{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python373jvsc74a57bd09164a3399a70d355c381b62813f30880ed90ca5a6f321bf0d85375640bda7ee5",
   "display_name": "Python 3.7.3 64-bit"
  },
  "metadata": {
   "interpreter": {
    "hash": "9164a3399a70d355c381b62813f30880ed90ca5a6f321bf0d85375640bda7ee5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "import keras\n",
    "import sklearn\n",
    "import pandas\n",
    "from time import time\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import load_model\n",
    "from keras.layers import *\n",
    "from keras import layers\n",
    "from keras import Model\n",
    "from keras import optimizers\n",
    "from keras import regularizers\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras import optimizers\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.applications import *\n",
    "from sklearn.metrics import classification_report\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 224\n",
    "DATA_PATH = 'F:/Machine_Learning/Projects/paper_implementation/Paper_2_GoogleNet/Peliculas/Dataframes'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "c = 0\n",
    "c < 4791"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "violent_frames = []\n",
    "non_violent_frames = []\n",
    "c = 0\n",
    "for frames in os.listdir(DATA_PATH):\n",
    "    frame = cv2.resize(cv2.imread(os.path.join(DATA_PATH,frames)), (IMG_SIZE, IMG_SIZE))\n",
    "\n",
    "    if c < 4791:\n",
    "        violent_frames.append(frame)\n",
    "    elif c >= 4791:\n",
    "        non_violent_frames.append(frame)\n",
    "    c += 1    \n",
    "\n",
    "np.save('movie_data_for_raw_model_violent.npy', violent_frames)\n",
    "np.save('movie_data_for_raw_model_nonviolent.npy', non_violent_frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "4791\n5050\n"
     ]
    }
   ],
   "source": [
    "print(len(violent_frames))\n",
    "print(len(non_violent_frames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+++ Extracting feature... +++\n",
      "Before Feature extraction: \n",
      "(4791, 224, 224, 3) (5050, 224, 224, 3)\n",
      "Adding all data:  (9841, 224, 224, 3)\n"
     ]
    }
   ],
   "source": [
    "print(\"+++ Extracting feature... +++\")\n",
    "    \n",
    "violent_frames = np.asarray(violent_frames)\n",
    "non_violent_frames = np.asarray(non_violent_frames)\n",
    "\n",
    "print (\"Before Feature extraction: \")\n",
    "print(violent_frames.shape,non_violent_frames.shape)\n",
    "all_data = np.vstack((violent_frames,non_violent_frames))\n",
    "print(\"Adding all data: \", all_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = resnet50.ResNet50(input_shape=(224,224,3), include_top=False)\n",
    "loaded_model = Model(loaded_model.input,loaded_model.output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "985/985 [==============================] - 439s 445ms/step\n"
     ]
    }
   ],
   "source": [
    "with tf.device('/cpu'):\n",
    "    features = loaded_model.predict(all_data,batch_size=10,verbose=1)\n",
    "    np.save('resnet__movie_feature.npy', np.asarray(features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(9841, 7, 7, 2048)\n"
     ]
    }
   ],
   "source": [
    "print(features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Violent features:  (4770, 7, 7, 2048)\nNon Violent features:  (4770, 7, 7, 2048)\n"
     ]
    }
   ],
   "source": [
    "violent_features = features[0:4770]\n",
    "non_violent_features = features[-4770:]\n",
    "print(\"Violent features: \", violent_features.shape)\n",
    "print(\"Non Violent features: \", non_violent_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "violent_vid = []\n",
    "non_violent_vid = []\n",
    "\n",
    "i = 0\n",
    "while i < len(violent_features):\n",
    "    violent_vid.append(np.array(violent_features[i:i+30]))\n",
    "    i = i+30\n",
    "\n",
    "i = 0\n",
    "while i < len(non_violent_features):\n",
    "    non_violent_vid.append(np.array(non_violent_features[i:i+30]))\n",
    "    i = i+30\n",
    "\n",
    "violent_vid = np.asarray(violent_vid)\n",
    "non_violent_vid = np.asarray(non_violent_vid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "violent_y, non_violent_y = np.zeros(len(violent_vid)), np.ones(len(non_violent_vid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Violent Video Seq:  (159, 30, 7, 7, 2048) Non_violent video Seq:  (159, 30, 7, 7, 2048)\nViolent Label:  (159,) Non_violent Label:  (159,)\n"
     ]
    }
   ],
   "source": [
    "#Special Case for this paper only. \n",
    "#Violent = 0\n",
    "#Non violent = 1\n",
    "\n",
    "print(\"Violent Video Seq: \", violent_vid.shape,\"Non_violent video Seq: \", non_violent_vid.shape)\n",
    "print(\"Violent Label: \", violent_y.shape, \"Non_violent Label: \", non_violent_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(318, 30, 100352)\n(318,)\n"
     ]
    }
   ],
   "source": [
    "X = np.vstack((violent_vid,non_violent_vid))\n",
    "y = np.append(violent_y, non_violent_y)\n",
    "X = np.reshape(X,(X.shape[0],X.shape[1],X.shape[2]*X.shape[3]*X.shape[4]))\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"sequential_1\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ncu_dnnlstm (CuDNNLSTM)       (None, 50)                20080800  \n_________________________________________________________________\ndense (Dense)                (None, 1)                 51        \n=================================================================\nTotal params: 20,080,851\nTrainable params: 20,080,851\nNon-trainable params: 0\n_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = load_model('./resnet_LSTM_model.h5')\n",
    "model.load_weights('./resnet_LSTM_weights.h5')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "64/64 [==============================] - 2s 28ms/step - loss: 1.8387 - accuracy: 0.3931\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[1.8386973142623901, 0.39308175444602966]"
      ]
     },
     "metadata": {},
     "execution_count": 28
    }
   ],
   "source": [
    "model.evaluate(x = X, y =y, batch_size= 5, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}